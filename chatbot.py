# -*- coding: utf-8 -*-
"""Mental_Health_Chatbot_GEN_AI_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nvKQNsA9N9wX4CfKQVFn6r9KxNC1gQV9
"""

!pip install langchain_groq langchain_core langchain_community

from langchain_groq import ChatGroq
llm=ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
result=llm.invoke("What is the most populated country in the world?")
print(result.content)

!pip install pypdf

!pip install sentence_transformers

!pip install chromadb

from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
def initialize_llm():
  llm=ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
  return llm

def create_vector_db():
    loader = DirectoryLoader("/content/sample_data/", glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()

    if not documents:
        print("No PDF documents found in the directory.")
        return None

    # Split documents into smaller chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)
    embeddings = HuggingFaceBgeEmbeddings(model_name="BAAI/bge-small-en-v1.5")

    # Create and persist the vector database
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')
    vector_db.persist()
    print("ChromaDB Created and Saved")

    return vector_db

def setup_qa_chain(vector_db, llm):
  retriever = vector_db.as_retriever()
  prompt_templates = """ You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
    {context}
    User: {question}
    Chatbot: """
  PROMPT = PromptTemplate(template = prompt_templates, input_variables = ['context', 'question'])

  qa_chain = RetrievalQA.from_chain_type(
      llm = llm,
      chain_type = "stuff",
      retriever = retriever,
      chain_type_kwargs = {"prompt": PROMPT}
  )
  return qa_chain

def main():
  print("Intializing Chatbot.........")
  llm = initialize_llm()

  db_path = "/content/chroma_db"

  if not os.path.exists(db_path):
    vector_db  = create_vector_db()
  else:
    embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
  qa_chain = setup_qa_chain(vector_db, llm)

  while True:
    query = input("\nHuman: ")
    if query.lower()  == "exit":
      print("Chatbot: Take Care of yourself, Goodbye!")
      break
    response = qa_chain.run(query)
    print(f"Chatbot: {response}")

if __name__ == "__main__":
  main()

!pip install gradio

!pip install langchain-chroma

!pip install transformers torch

import gradio as gr
import os
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_chroma import Chroma
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq

def initialize_llm():
    try:
        llm = ChatGroq(
            temperature=0,
            groq_api_key="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",
            model_name="llama-3.3-70b-versatile"
        )
        print("LLM initialized successfully.")
        return llm
    except Exception as e:
        print(f"Error initializing LLM: {e}")
        raise

def create_vector_db():
    try:
        loader = DirectoryLoader("/content/sample_data/", glob="*.pdf", loader_cls=PyPDFLoader)
        documents = loader.load()

        if not documents:
            raise ValueError("No PDF documents found in the directory /content/sample_data/. Please ensure the directory contains PDF files.")

        print(f"Loaded {len(documents)} documents.")

        # Split documents into smaller chunks
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        texts = text_splitter.split_documents(documents)
        print(f"Split documents into {len(texts)} chunks.")

        embeddings = HuggingFaceBgeEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

        # Create and persist the vector database
        vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')
        vector_db.persist()
        print("ChromaDB created and data saved.")

        return vector_db
    except Exception as e:
        print(f"Error creating vector database: {e}")
        raise

def setup_retriever(vector_db):
    try:
        retriever = vector_db.as_retriever(search_kwargs={"k": 4})  # Retrieve top 4 documents
        print("Retriever set up successfully.")
        return retriever
    except Exception as e:
        print(f"Error setting up retriever: {e}")
        raise

# Initialize the chatbot
print("Initializing Chatbot...")
try:
    llm = initialize_llm()
except Exception as e:
    print(f"Failed to initialize chatbot: {e}")
    exit(1)

db_path = "/content/chroma_db"

try:
    if not os.path.exists(db_path):
        vector_db = create_vector_db()
    else:
        embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
        vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
        print("Loaded existing ChromaDB.")
except Exception as e:
    print(f"Failed to initialize vector database: {e}")
    exit(1)

# Validate vector database
try:
    doc_count = vector_db._collection.count()
    if doc_count == 0:
        raise ValueError("Vector database is empty. Please ensure that documents were loaded correctly.")
    print(f"Vector database contains {doc_count} documents.")
except Exception as e:
    print(f"Error validating vector database: {e}")
    exit(1)

try:
    retriever = setup_retriever(vector_db)
except Exception as e:
    print(f"Failed to set up retriever: {e}")
    exit(1)

# Test the retriever independently
try:
    test_query = "I'm Sad"
    retrieved_docs = retriever.get_relevant_documents(test_query)
    if not retrieved_docs:
        print("Warning: Retriever did not find any relevant documents for the query 'I'm Sad'.")
    else:
        print(f"Retriever found {len(retrieved_docs)} relevant documents for the query 'I'm Sad':")
        for i, doc in enumerate(retrieved_docs):
            print(f"Document {i+1}: {doc.page_content[:100]}...")
except Exception as e:
    print(f"Error testing retriever: {e}")

# Custom prompt template for mental health support
prompt_template = """You are a compassionate mental health chatbot. Your goal is to provide emotional support, listen empathetically, and offer thoughtful responses based on the user's input and the provided context. Use the following context to inform your response, but ensure your tone remains warm, understanding, and non-judgmental. If the context is not sufficient, provide a general supportive response and encourage the user to share more.

Context: {context}

Chat History: {chat_history}

User: {question}

Chatbot: """
PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'chat_history', 'question'])

def chatbot_response(user_input, history=None):
    if history is None:
        history = []
    if not user_input.strip():
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": "Please provide a valid input."})
        return "", history

    try:
        # Log the input
        print(f"Processing user input: {user_input}")

        # Retrieve relevant documents
        retrieved_docs = retriever.get_relevant_documents(user_input)
        if not retrieved_docs:
            print("No relevant documents found for the query.")
            context = "No relevant information available."
            response = "I'm sorry, I couldn't find any relevant information to help with your query. Can you tell me more about how you're feeling?"
        else:
            print(f"Found {len(retrieved_docs)} relevant documents.")
            for i, doc in enumerate(retrieved_docs):
                print(f"Retrieved Document {i+1}: {doc.page_content[:200]}...")
            context = "\n".join([doc.page_content for doc in retrieved_docs])

        # Construct chat history
        chat_history = ""
        for msg in history:
            role = msg["role"]
            content = msg["content"]
            chat_history += f"{role.capitalize()}: {content}\n"

        # Format the prompt
        prompt = PROMPT.format(context=context, chat_history=chat_history, question=user_input)

        # Generate response using the LLM
        response = llm.invoke(prompt).content  # Use .content to get the string response from ChatGroq
        print(f"LLM response: {response}")

        # Ensure the history is in the correct format
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": response})
        print(f"Returning history: {history}")
        return "", history
    except Exception as e:
        print(f"Error in chatbot_response: {e}")
        error_message = f"Sorry, I encountered an error while processing your request: {str(e)}"
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": error_message})
        print(f"Returning history with error: {history}")
        return "", history

# Gradio interface
with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
    gr.Markdown("# 🧠 Mental Health Chatbot 🤖")
    gr.Markdown("A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.")

    # Try a simpler Chatbot component to isolate the issue
    chatbot = gr.Chatbot(label="Mental Health Chatbot", type="messages")
    user_input = gr.Textbox(label="Your Message", placeholder="Type your message here...")
    submit_button = gr.Button("Submit")

    def chat_interface_wrapper(user_input, history):
        print(f"Received user input: {user_input}, history: {history}")
        _, updated_history = chatbot_response(user_input, history)
        print(f"Updated history: {updated_history}")
        return updated_history, ""

    submit_button.click(
        fn=chat_interface_wrapper,
        inputs=[user_input, chatbot],
        outputs=[chatbot, user_input]
    )

    gr.Markdown("This chatbot provides general support. For urgent issues, seek help from licensed professionals.")

app.launch(debug=True)

